{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Injury Data from https://www.prosportstransactions.com/\n",
    "### Code adopted from:\n",
    "- https://github.com/gboogy/nba-injury-data-scraper\n",
    "- https://github.com/elap733/NBA-Injuries-Analysis/blob/master/src/d01_scrapes/scrape_missedgames.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from thefuzz import fuzz, process\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed, Retrying\n",
    "\n",
    "pd.options.mode.chained_assignment =  None\n",
    "\n",
    "data_DIR = \"../../data/injuries/\"\n",
    "export_DIR = \"../../../repos/csv/\"\n",
    "# Pretending to be a browser\n",
    "header = {\n",
    "  \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "  \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "}\n",
    "\n",
    "with open(\"../../data/NBA.json\") as f:\n",
    "    data = json.load(f)\n",
    "pID_dict = {v: int(k) for k, v in data.items()}\n",
    "player_dict = {int(k): v for k, v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Date\n",
    "start_date = \"2023-06-01\"\n",
    "start_date = \"2023-10-24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df0 = pd.read_parquet(data_DIR + \"NBA_prosptran_injuries_2023.parquet1\")\n",
    "    start_date = (df0[\"Date\"].iloc[-1] + dt.timedelta(days=-1)).strftime(\"%Y-%m-%d\")\n",
    "except:\n",
    "    df0 = pd.DataFrame()\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including these following conditions:\n",
    "- Movement to/from injured/inactive list (IL)\n",
    "- Missed games due to injury\n",
    "- Missed games due to personal reasons\n",
    "- Missed games due to suspensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL to scrape from \n",
    "url = f\"https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate={start_date}&EndDate=&ILChkBx=yes&InjuriesChkBx=yes&PersonalChkBx=yes&DisciplinaryChkBx=yes&Submit=Search\"\n",
    "\n",
    "url = f\"https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate={start_date}&EndDate=&ILChkBx=yes&InjuriesChkBx=yes&Submit=Search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------Scrape web page--------------------------------------\n",
    "\n",
    "#Get URL HTML\n",
    "response = requests.get(url)\n",
    "print(response) # Response [200] means it went through\n",
    "\n",
    "#Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#-------------Scrape data from the first web page----------------\n",
    "#Read in html as pandas data frame\n",
    "df_first_page = pd.read_html(url,storage_options=header)\n",
    "    \n",
    "#Select table of interest (the first table)\n",
    "df_first_page = df_first_page[0]\n",
    "\n",
    "#Drop first row (column names)\n",
    "df_first_page.drop([0], inplace = True)\n",
    "   \n",
    "#Remove bullet in front of player names\n",
    "df_first_page[2]=df_first_page[2].str[2:] # \"Acquired\" column\n",
    "df_first_page[3]=df_first_page[3].str[2:] # \"Relinquished\" column\n",
    "    \n",
    "#Modify column titles\n",
    "df_first_page.columns = ['Date','Team','Acquired','Relinquished','Notes']\n",
    "\n",
    "dfa = []\n",
    "#data frame list to hold data for concating later\n",
    "dfa.append(df_first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------Scrape data from other pages linked at the bottom of the first page------------\n",
    "# Loop over links (skipping the first 4 (not data) and last 4 (\"Next\" and other webpage links))\n",
    "for i in tqdm(range(4,len(soup.findAll('a'))-4)): #'a' tags are for links\n",
    "   \n",
    "    #find all links on webpage and select the i-th link\n",
    "    one_a_tag = soup.findAll('a')[i]\n",
    "    link = one_a_tag['href']\n",
    "    \n",
    "    #Add in the rest of the url\n",
    "    download_url = 'https://www.prosportstransactions.com/basketball/Search/'+ link\n",
    "    # print(download_url)\n",
    "    \n",
    "    #Read html as pandas data frame\n",
    "    dfs = pd.read_html(download_url, storage_options=header)\n",
    "    \n",
    "    #Select table of interest (the first table)\n",
    "    df = dfs[0]\n",
    "    \n",
    "    #Drop first row (column names)\n",
    "    df.drop([0], inplace = True)\n",
    "   \n",
    "    #Remove bullet in front of names\n",
    "    df[2]=df[2].str[2:] # \"Acquired\" column\n",
    "    df[3]=df[3].str[2:] # \"Relinquished\" column\n",
    "    \n",
    "    #Modify column titles\n",
    "    df.columns = ['Date','Team','Acquired','Relinquished','Notes']\n",
    "    #Add a pause to keep web server happy\n",
    "    time.sleep(0.2)\n",
    "    dfa.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_pId(player,player_dict):\n",
    "    # frat = [[v,fuzz.ratio(player,k)] for k, v in pID_dict.items()]\n",
    "    # frar  = np.array(frat).T\n",
    "    # pId = frar[:,frar.argmax(axis=1)[1]][0]\n",
    "    pId = process.extract(player,player_dict,limit=1, scorer=fuzz.partial_ratio)[0][2]\n",
    "    return pId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat(dfa)\n",
    "df = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq = df['Acquired']\n",
    "rel = df['Relinquished']\n",
    "df['Acquired'] = np.where(\n",
    "    acq.str.contains('/'), acq.str.split('/ ').str[1], acq)\n",
    "df['Relinquished'] = np.where(\n",
    "    rel.str.contains('/'), rel.str.split('/ ').str[1], rel)\n",
    "\n",
    "# Remove instances where value is like \"(some text)\"\n",
    "df['Acquired'] = df.Acquired.str.replace(\n",
    "    r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "df['Relinquished'] = df.Relinquished.str.replace(\n",
    "    r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "df[\"In\"] = ~df[\"Acquired\"].isna()\n",
    "df[\"Out\"] = ~df[\"Relinquished\"].isna()\n",
    "df[\"Player\"] =  (df[\"Acquired\"]*~df[\"Acquired\"].isna()).fillna(\"\") +\\\n",
    "                (df[\"Relinquished\"]*~df[\"Relinquished\"].isna()).fillna(\"\")\n",
    "df = df[[\"Date\",\"Team\",\"Player\",\"In\",\"Out\",\"Notes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df[\"Player\"].str.istitle()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"playerID\"] = df[\"Player\"].map(pID_dict)\n",
    "df1 = df.copy()\n",
    "df1[\"playerID\"][df[\"playerID\"].isna()] = df[\"Player\"][df[\"playerID\"].isna()].apply(lambda x: get_missing_pId(x,player_dict))\n",
    "df1[\"playerID\"] = df1[\"playerID\"].astype(int)\n",
    "df1[\"Date\"] = pd.to_datetime(df1[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df1.insert(2,\"playerID\",df1.pop(\"playerID\"))\n",
    "df2 = pd.concat([df0,df1]).reset_index(drop=True)\n",
    "df3 =df2[~df2.duplicated(keep='last')].reset_index(drop=True)\n",
    "df3 = df3[~df3[\"Notes\"].str.contains(\"fine\",case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.query(\"Team == 'Timberwolves'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.to_csv(data_DIR + 'NBA_prosptran_injuries_2023.csv', index=False)\n",
    "df3.to_parquet(data_DIR + 'NBA_prosptran_injuries_only_2023.parquet')\n",
    "# df3.to_csv(export_DIR + 'NBA_prosptran_injuries_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df3.query(\"Player == 'Tyler Herro'\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did Herro Miss the game on 1st November?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_date = pd.to_datetime(dt.date(2023,11,1))\n",
    "dfp[\"Comp\"] = dfp[\"Date\"] <= game_date\n",
    "idxi = dfp[dfp[\"Comp\"]].index\n",
    "if len(idxi) > 0:\n",
    "    idx = idxi[-1]\n",
    "    missed_game = dfp[\"Out\"].loc[idx]\n",
    "else:\n",
    "    missed_game = False\n",
    "missed_game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older Seasons Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfdsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_pId(player,player_dict):\n",
    "    pId = process.extract(player,player_dict,limit=1, scorer=fuzz.partial_ratio)[0][2]\n",
    "    return pId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @retry(stop=stop_after_attempt(5), wait=wait_fixed(0.6))\n",
    "def update_injury_data(year):\n",
    "\n",
    "    header = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "    start_date = f\"{year}-07-01\"\n",
    "    end_date = f\"{year+1}-06-30\"\n",
    "    try:\n",
    "        # raise Exception\n",
    "        df0 = pd.read_parquet(data_DIR + f'NBA_prosptran_injuries_{year}.parquet')\n",
    "        start_date = (df0[\"Date\"].iloc[-1] + dt.timedelta(days=-1)).strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        df0 = pd.DataFrame()\n",
    "        \n",
    "    print(start_date)\n",
    "    url = f\"https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate={start_date}&EndDate={end_date}&ILChkBx=yes&InjuriesChkBx=yes&PersonalChkBx=yes&Submit=Search\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    # print(response) # Response [200] means it went through\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    df_first_page = pd.read_html(url,storage_options=header)\n",
    "    df_first_page = df_first_page[0]\n",
    "    df_first_page.drop([0], inplace = True)\n",
    "    df_first_page[2]=df_first_page[2].str[2:] # \"Acquired\" column\n",
    "    df_first_page[3]=df_first_page[3].str[2:] # \"Relinquished\" column\n",
    "    df_first_page.columns = ['Date','Team','Acquired','Relinquished','Notes']\n",
    "    dfa = []\n",
    "    dfa.append(df_first_page)\n",
    "    for i in tqdm(range(4,len(soup.findAll('a'))-4)): #'a' tags are for links\n",
    "        for kk in Retrying(wait=wait_fixed(5)):\n",
    "            try: \n",
    "                tic = time.perf_counter()\n",
    "                one_a_tag = soup.findAll('a')[i]\n",
    "                link = one_a_tag['href']\n",
    "                download_url = 'https://www.prosportstransactions.com/basketball/Search/'+ link\n",
    "                # print(download_url)\n",
    "                dfs = pd.read_html(download_url, storage_options=header)\n",
    "                df = dfs[0]\n",
    "                df.drop([0], inplace = True)\n",
    "                df[2]=df[2].str[2:] # \"Acquired\" column\n",
    "                df[3]=df[3].str[2:] # \"Relinquished\" column\n",
    "                df.columns = ['Date','Team','Acquired','Relinquished','Notes']\n",
    "                toc = time.perf_counter()\n",
    "                if (toc - tic) >10:\n",
    "                    raise Exception(\"Website Timeout\")\n",
    "                time.sleep(0.2)\n",
    "                dfa.append(df)\n",
    "                break\n",
    "            except Exception as error:\n",
    "                 print(download_url)\n",
    "                 print(error)\n",
    "                 continue\n",
    "\n",
    "    df1 = pd.concat(dfa)\n",
    "    df = df1.copy()\n",
    "    acq = df['Acquired']\n",
    "    rel = df['Relinquished']\n",
    "    df['Acquired'] = np.where(\n",
    "        acq.str.contains('/'), acq.str.split('/ ').str[1], acq)\n",
    "    df['Relinquished'] = np.where(\n",
    "        rel.str.contains('/'), rel.str.split('/ ').str[1], rel)\n",
    "\n",
    "    # Remove instances where value is like \"(some text)\"\n",
    "    df['Acquired'] = df.Acquired.str.replace(\n",
    "        r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "    df['Relinquished'] = df.Relinquished.str.replace(\n",
    "        r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "    df[\"In\"] = ~df[\"Acquired\"].isna()\n",
    "    df[\"Out\"] = ~df[\"Relinquished\"].isna()\n",
    "    df[\"Player\"] =  (df[\"Acquired\"]*~df[\"Acquired\"].isna()).fillna(\"\") +\\\n",
    "                    (df[\"Relinquished\"]*~df[\"Relinquished\"].isna()).fillna(\"\")\n",
    "    df = df[[\"Date\",\"Team\",\"Player\",\"In\",\"Out\",\"Notes\"]]\n",
    "    df = df[df[\"Player\"].str.istitle()].reset_index(drop=True)\n",
    "    df[\"Player\"].loc[df[\"Player\"].str.contains(\"Enes\")] = \"Enes Kanter\"\n",
    "    df[\"playerID\"] = df[\"Player\"].map(pID_dict)\n",
    "    df1 = df.copy()\n",
    "    df1[\"playerID\"][df[\"playerID\"].isna()] = df[\"Player\"][df[\"playerID\"].isna()].apply(lambda x: get_missing_pId(x,player_dict))\n",
    "    df1[\"playerID\"] = df1[\"playerID\"].astype(int)\n",
    "    df1[\"Date\"] = pd.to_datetime(df1[\"Date\"], format=\"%Y-%m-%d\")\n",
    "    df1.insert(2,\"playerID\",df1.pop(\"playerID\"))\n",
    "    df2 = pd.concat([df0,df1]).reset_index(drop=True)\n",
    "    df3 =df2[~df2.duplicated(keep='last')].reset_index(drop=True)\n",
    "    df3.to_csv(data_DIR + f'NBA_prosptran_injuries_{year}.csv', index=False)\n",
    "    df3.to_parquet(data_DIR + f'NBA_prosptran_injuries_{year}.parquet')\n",
    "\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2000,2024):\n",
    "    dfy = update_injury_data(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_pId(player, player_dict):\n",
    "    pId = process.extract(player, player_dict, scorer=fuzz.partial_ratio, limit=1)[0][2]\n",
    "    return pId\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "injury_DIR = data_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = int(season)\n",
    "header = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "}\n",
    "start_date = f\"{year}-07-01\"\n",
    "end_date = f\"{year+1}-06-30\"\n",
    "df0 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:31<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "print(start_date)\n",
    "url = f\"https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate={start_date}&EndDate={end_date}&ILChkBx=yes&InjuriesChkBx=yes&PersonalChkBx=yes&Submit=Search\"\n",
    "\n",
    "response = requests.get(url)\n",
    "# print(response) # Response [200] means it went through\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "df_first_page = pd.read_html(url, storage_options=header)\n",
    "df_first_page = df_first_page[0]\n",
    "df_first_page.drop([0], inplace=True)\n",
    "df_first_page[2] = df_first_page[2].str[2:]  # \"Acquired\" column\n",
    "df_first_page[3] = df_first_page[3].str[2:]  # \"Relinquished\" column\n",
    "df_first_page.columns = [\"Date\", \"Team\", \"Acquired\", \"Relinquished\", \"Notes\"]\n",
    "dfa = []\n",
    "dfa.append(df_first_page)\n",
    "for i in tqdm(range(4, len(soup.findAll(\"a\")) - 4)):  #'a' tags are for links\n",
    "    for kk in Retrying(wait=wait_fixed(5)):\n",
    "        try:\n",
    "            tic = time.perf_counter()\n",
    "            one_a_tag = soup.findAll(\"a\")[i]\n",
    "            link = one_a_tag[\"href\"]\n",
    "            download_url = (\n",
    "                \"https://www.prosportstransactions.com/basketball/Search/\"\n",
    "                + link\n",
    "            )\n",
    "            # print(download_url)\n",
    "            dfs = pd.read_html(download_url, storage_options=header)\n",
    "            df = dfs[0]\n",
    "            df.drop([0], inplace=True)\n",
    "            df[2] = df[2].str[2:]  # \"Acquired\" column\n",
    "            df[3] = df[3].str[2:]  # \"Relinquished\" column\n",
    "            df.columns = [\"Date\", \"Team\", \"Acquired\", \"Relinquished\", \"Notes\"]\n",
    "            toc = time.perf_counter()\n",
    "            if (toc - tic) > 10:\n",
    "                raise Exception(\"Website Timeout\")\n",
    "            time.sleep(0.2)\n",
    "            dfa.append(df)\n",
    "            break\n",
    "        except Exception as error:\n",
    "            print(download_url)\n",
    "            print(error)\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.concat(dfa)\n",
    "df = df10.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq = df[\"Acquired\"]\n",
    "rel = df[\"Relinquished\"]\n",
    "df[\"Acquired\"] = np.where(\n",
    "    acq.str.contains(\"/\"), acq.str.split(\"/ \").str[1], acq\n",
    ")\n",
    "df[\"Relinquished\"] = np.where(\n",
    "    rel.str.contains(\"/\"), rel.str.split(\"/ \").str[1], rel\n",
    ")\n",
    "\n",
    "# Remove instances where value is like \"(some text)\"\n",
    "df[\"Acquired\"] = df.Acquired.str.replace(r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "df[\"Relinquished\"] = df.Relinquished.str.replace(r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "df[\"In\"] = ~df[\"Acquired\"].isna()\n",
    "df[\"Out\"] = ~df[\"Relinquished\"].isna()\n",
    "df[\"Player\"] = (df[\"Acquired\"] * ~df[\"Acquired\"].isna()).fillna(\"\") + (\n",
    "    df[\"Relinquished\"] * ~df[\"Relinquished\"].isna()\n",
    ").fillna(\"\")\n",
    "df = df[[\"Date\", \"Team\", \"Player\", \"In\", \"Out\", \"Notes\"]]\n",
    "# df = df[df[\"Player\"].str.istitle()].reset_index(drop=True)\n",
    "df[\"Player\"].loc[df[\"Player\"].str.contains(\"Enes\")] = \"Enes Kanter\"\n",
    "df[\"playerID\"] = df[\"Player\"].map(pID_dict)\n",
    "df1 = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Team</th>\n",
       "      <th>Player</th>\n",
       "      <th>In</th>\n",
       "      <th>Out</th>\n",
       "      <th>Notes</th>\n",
       "      <th>playerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-11</td>\n",
       "      <td>Kings</td>\n",
       "      <td>Devin Carter</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>surgery on left shoulder to repair labrum (out...</td>\n",
       "      <td>1642269.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-13</td>\n",
       "      <td>Nuggets</td>\n",
       "      <td>DaRon Holmes II</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>torn right Achilles tendon (out indefinitely)</td>\n",
       "      <td>1641747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-17</td>\n",
       "      <td>Bucks</td>\n",
       "      <td>Khris Middleton</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>arthroscopic surgery on both ankles (out indef...</td>\n",
       "      <td>203114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-09-09</td>\n",
       "      <td>Lakers</td>\n",
       "      <td>Christian Wood</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>surgery on left knee (out indefinitely)</td>\n",
       "      <td>1626174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-09-17</td>\n",
       "      <td>Thunder</td>\n",
       "      <td>Kenrich Williams</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>placed on IL with surgery on right knee</td>\n",
       "      <td>1629026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>Pelicans</td>\n",
       "      <td>Jeremiah Robinson-Earl</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>sprained left ankle (DTD)</td>\n",
       "      <td>1630526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>Raptors</td>\n",
       "      <td>R.J. Barrett</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undisclosed (DTD)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>Suns</td>\n",
       "      <td>Royce O'Neale</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>sprained left ankle (DTD)</td>\n",
       "      <td>1626220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>Warriors</td>\n",
       "      <td>Moses Moody</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>returned to lineup</td>\n",
       "      <td>1630541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>Pelicans</td>\n",
       "      <td>Larry Nance Jr.</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>surgery on right hand to repair fractured four...</td>\n",
       "      <td>1626204.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Team                  Player     In    Out  \\\n",
       "1   2024-07-11     Kings            Devin Carter  False   True   \n",
       "2   2024-07-13   Nuggets         DaRon Holmes II  False   True   \n",
       "3   2024-07-17     Bucks         Khris Middleton  False   True   \n",
       "4   2024-09-09    Lakers          Christian Wood  False   True   \n",
       "5   2024-09-17   Thunder        Kenrich Williams  False   True   \n",
       "..         ...       ...                     ...    ...    ...   \n",
       "2   2024-12-30  Pelicans  Jeremiah Robinson-Earl  False   True   \n",
       "3   2024-12-30   Raptors            R.J. Barrett  False   True   \n",
       "4   2024-12-30      Suns           Royce O'Neale  False   True   \n",
       "5   2024-12-30  Warriors             Moses Moody   True  False   \n",
       "6   2025-01-02  Pelicans         Larry Nance Jr.  False   True   \n",
       "\n",
       "                                                Notes   playerID  \n",
       "1   surgery on left shoulder to repair labrum (out...  1642269.0  \n",
       "2       torn right Achilles tendon (out indefinitely)  1641747.0  \n",
       "3   arthroscopic surgery on both ankles (out indef...   203114.0  \n",
       "4             surgery on left knee (out indefinitely)  1626174.0  \n",
       "5             placed on IL with surgery on right knee  1629026.0  \n",
       "..                                                ...        ...  \n",
       "2                           sprained left ankle (DTD)  1630526.0  \n",
       "3                                   undisclosed (DTD)        NaN  \n",
       "4                           sprained left ankle (DTD)  1626220.0  \n",
       "5                                  returned to lineup  1630541.0  \n",
       "6   surgery on right hand to repair fractured four...  1626204.0  \n",
       "\n",
       "[1055 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1[\"playerID\"] = df1[\"playerID\"].astype(int)\n",
    "df1[\"Date\"] = pd.to_datetime(df1[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df1.insert(2, \"playerID\", df1.pop(\"playerID\"))\n",
    "df2 = pd.concat([df0, df1]).reset_index(drop=True)\n",
    "df3 = df2[~df2.duplicated(keep=\"last\")].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        try:\n",
    "            # raise Exception\n",
    "            df0 = pd.read_parquet(\n",
    "                injury_DIR + f\"NBA_prosptran_injuries_{year}.parquet\"\n",
    "            )\n",
    "            start_date = (df0[\"Date\"].iloc[-1] + dt.timedelta(days=-1)).strftime(\n",
    "                \"%Y-%m-%d\"\n",
    "            )\n",
    "        except:\n",
    "            df0 = pd.DataFrame()\n",
    "\n",
    "        print(start_date)\n",
    "        url = f\"https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate={start_date}&EndDate={end_date}&ILChkBx=yes&InjuriesChkBx=yes&PersonalChkBx=yes&Submit=Search\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        # print(response) # Response [200] means it went through\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        df_first_page = pd.read_html(url, storage_options=header)\n",
    "        df_first_page = df_first_page[0]\n",
    "        df_first_page.drop([0], inplace=True)\n",
    "        df_first_page[2] = df_first_page[2].str[2:]  # \"Acquired\" column\n",
    "        df_first_page[3] = df_first_page[3].str[2:]  # \"Relinquished\" column\n",
    "        df_first_page.columns = [\"Date\", \"Team\", \"Acquired\", \"Relinquished\", \"Notes\"]\n",
    "        dfa = []\n",
    "        dfa.append(df_first_page)\n",
    "        for i in tqdm(range(4, len(soup.findAll(\"a\")) - 4)):  #'a' tags are for links\n",
    "            for kk in Retrying(wait=wait_fixed(5)):\n",
    "                try:\n",
    "                    tic = time.perf_counter()\n",
    "                    one_a_tag = soup.findAll(\"a\")[i]\n",
    "                    link = one_a_tag[\"href\"]\n",
    "                    download_url = (\n",
    "                        \"https://www.prosportstransactions.com/basketball/Search/\"\n",
    "                        + link\n",
    "                    )\n",
    "                    # print(download_url)\n",
    "                    dfs = pd.read_html(download_url, storage_options=header)\n",
    "                    df = dfs[0]\n",
    "                    df.drop([0], inplace=True)\n",
    "                    df[2] = df[2].str[2:]  # \"Acquired\" column\n",
    "                    df[3] = df[3].str[2:]  # \"Relinquished\" column\n",
    "                    df.columns = [\"Date\", \"Team\", \"Acquired\", \"Relinquished\", \"Notes\"]\n",
    "                    toc = time.perf_counter()\n",
    "                    if (toc - tic) > 10:\n",
    "                        raise Exception(\"Website Timeout\")\n",
    "                    time.sleep(0.2)\n",
    "                    dfa.append(df)\n",
    "                    break\n",
    "                except Exception as error:\n",
    "                    print(download_url)\n",
    "                    print(error)\n",
    "                    continue\n",
    "\n",
    "        df1 = pd.concat(dfa)\n",
    "        df = df1.copy()\n",
    "        acq = df[\"Acquired\"]\n",
    "        rel = df[\"Relinquished\"]\n",
    "        df[\"Acquired\"] = np.where(\n",
    "            acq.str.contains(\"/\"), acq.str.split(\"/ \").str[1], acq\n",
    "        )\n",
    "        df[\"Relinquished\"] = np.where(\n",
    "            rel.str.contains(\"/\"), rel.str.split(\"/ \").str[1], rel\n",
    "        )\n",
    "\n",
    "        # Remove instances where value is like \"(some text)\"\n",
    "        df[\"Acquired\"] = df.Acquired.str.replace(r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "        df[\"Relinquished\"] = df.Relinquished.str.replace(r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "        df[\"In\"] = ~df[\"Acquired\"].isna()\n",
    "        df[\"Out\"] = ~df[\"Relinquished\"].isna()\n",
    "        df[\"Player\"] = (df[\"Acquired\"] * ~df[\"Acquired\"].isna()).fillna(\"\") + (\n",
    "            df[\"Relinquished\"] * ~df[\"Relinquished\"].isna()\n",
    "        ).fillna(\"\")\n",
    "        df = df[[\"Date\", \"Team\", \"Player\", \"In\", \"Out\", \"Notes\"]]\n",
    "        # df = df[df[\"Player\"].str.istitle()].reset_index(drop=True)\n",
    "        df[\"Player\"].loc[df[\"Player\"].str.contains(\"Enes\")] = \"Enes Kanter\"\n",
    "        df[\"playerID\"] = df[\"Player\"].map(pID_dict)\n",
    "        df1 = df.copy()\n",
    "        df1.loc[df[\"playerID\"].isna(),\"playerID\"] = df1.loc[df[\"playerID\"].isna(),\"playerID\"].apply(lambda x: get_missing_pId(x, player_dict))\n",
    "        df1[\"playerID\"] = df1[\"playerID\"].astype(int)\n",
    "        df1[\"Date\"] = pd.to_datetime(df1[\"Date\"], format=\"%Y-%m-%d\")\n",
    "        df1.insert(2, \"playerID\", df1.pop(\"playerID\"))\n",
    "        df2 = pd.concat([df0, df1]).reset_index(drop=True)\n",
    "        df3 = df2[~df2.duplicated(keep=\"last\")].reset_index(drop=True)\n",
    "        df3.to_csv(csv_export_DIR + f\"NBA_prosptran_injuries_{year}.csv\", index=False)\n",
    "        df3.to_parquet(\n",
    "            injury_DIR + f\"NBA_prosptran_injuries_{year}.parquet\"\n",
    "        )\n",
    "        df3.to_parquet(\n",
    "            shiny_export_DIR2 + \"NBA-Games-Missed/\" + f\"NBA_prosptran_injuries_{year}.parquet\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba-ub9Z_EQq-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
