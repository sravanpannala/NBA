{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Injury Data from https://www.prosportstransactions.com/\n",
    "### Code adopted from:\n",
    "- https://github.com/gboogy/nba-injury-data-scraper\n",
    "- https://github.com/elap733/NBA-Injuries-Analysis/blob/master/src/d01_scrapes/scrape_missedgames.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Fix\n",
    "- 2022 some player index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from thefuzz import fuzz, process\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed, Retrying\n",
    "\n",
    "pd.options.mode.chained_assignment =  None\n",
    "\n",
    "data_DIR = \"../fdata/injuries/\"\n",
    "# Pretending to be a browser\n",
    "header = {\n",
    "  \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "  \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "}\n",
    "\n",
    "with open(\"../data/NBA.json\") as f:\n",
    "    data = json.load(f)\n",
    "data = data[\"players\"]\n",
    "pID_dict = {v: int(k) for k, v in data.items()}\n",
    "player_dict = {int(k): v for k, v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Date\n",
    "start_date = \"2023-06-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df0 = pd.read_parquet(data_DIR + \"NBA_prosptran_injuries_2023.parquet\")\n",
    "    start_date = (df0[\"Date\"].iloc[-1] + dt.timedelta(days=-1)).strftime(\"%Y-%m-%d\")\n",
    "except:\n",
    "    df0 = pd.DataFrame()\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including these following conditions:\n",
    "- Movement to/from injured/inactive list (IL)\n",
    "- Missed games due to injury\n",
    "- Missed games due to personal reasons\n",
    "- Missed games due to suspensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL to scrape from \n",
    "url = f\"https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate={start_date}&EndDate=&ILChkBx=yes&InjuriesChkBx=yes&PersonalChkBx=yes&DisciplinaryChkBx=yes&Submit=Search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------Scrape web page--------------------------------------\n",
    "\n",
    "#Get URL HTML\n",
    "response = requests.get(url)\n",
    "print(response) # Response [200] means it went through\n",
    "\n",
    "#Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#-------------Scrape data from the first web page----------------\n",
    "#Read in html as pandas data frame\n",
    "df_first_page = pd.read_html(url,storage_options=header)\n",
    "    \n",
    "#Select table of interest (the first table)\n",
    "df_first_page = df_first_page[0]\n",
    "\n",
    "#Drop first row (column names)\n",
    "df_first_page.drop([0], inplace = True)\n",
    "   \n",
    "#Remove bullet in front of player names\n",
    "df_first_page[2]=df_first_page[2].str[2:] # \"Acquired\" column\n",
    "df_first_page[3]=df_first_page[3].str[2:] # \"Relinquished\" column\n",
    "    \n",
    "#Modify column titles\n",
    "df_first_page.columns = ['Date','Team','Acquired','Relinquished','Notes']\n",
    "\n",
    "dfa = []\n",
    "#data frame list to hold data for concating later\n",
    "dfa.append(df_first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------Scrape data from other pages linked at the bottom of the first page------------\n",
    "# Loop over links (skipping the first 4 (not data) and last 4 (\"Next\" and other webpage links))\n",
    "for i in tqdm(range(4,len(soup.findAll('a'))-4)): #'a' tags are for links\n",
    "   \n",
    "    #find all links on webpage and select the i-th link\n",
    "    one_a_tag = soup.findAll('a')[i]\n",
    "    link = one_a_tag['href']\n",
    "    \n",
    "    #Add in the rest of the url\n",
    "    download_url = 'https://www.prosportstransactions.com/basketball/Search/'+ link\n",
    "    # print(download_url)\n",
    "    \n",
    "    #Read html as pandas data frame\n",
    "    dfs = pd.read_html(download_url, storage_options=header)\n",
    "    \n",
    "    #Select table of interest (the first table)\n",
    "    df = dfs[0]\n",
    "    \n",
    "    #Drop first row (column names)\n",
    "    df.drop([0], inplace = True)\n",
    "   \n",
    "    #Remove bullet in front of names\n",
    "    df[2]=df[2].str[2:] # \"Acquired\" column\n",
    "    df[3]=df[3].str[2:] # \"Relinquished\" column\n",
    "    \n",
    "    #Modify column titles\n",
    "    df.columns = ['Date','Team','Acquired','Relinquished','Notes']\n",
    "    #Add a pause to keep web server happy\n",
    "    time.sleep(0.2)\n",
    "    dfa.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_pId(player,player_dict):\n",
    "    # frat = [[v,fuzz.ratio(player,k)] for k, v in pID_dict.items()]\n",
    "    # frar  = np.array(frat).T\n",
    "    # pId = frar[:,frar.argmax(axis=1)[1]][0]\n",
    "    pId = process.extract(player,player_dict,limit=1)[0][2]\n",
    "    return pId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat(dfa)\n",
    "df = df1.copy()\n",
    "acq = df['Acquired']\n",
    "rel = df['Relinquished']\n",
    "df['Acquired'] = np.where(\n",
    "    acq.str.contains('/'), acq.str.split('/ ').str[1], acq)\n",
    "df['Relinquished'] = np.where(\n",
    "    rel.str.contains('/'), rel.str.split('/ ').str[1], rel)\n",
    "\n",
    "# Remove instances where value is like \"(some text)\"\n",
    "df['Acquired'] = df.Acquired.str.replace(\n",
    "    r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "df['Relinquished'] = df.Relinquished.str.replace(\n",
    "    r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "df[\"In\"] = ~df[\"Acquired\"].isna()\n",
    "df[\"Out\"] = ~df[\"Relinquished\"].isna()\n",
    "df[\"Player\"] =  (df[\"Acquired\"]*~df[\"Acquired\"].isna()).fillna(\"\") +\\\n",
    "                (df[\"Relinquished\"]*~df[\"Relinquished\"].isna()).fillna(\"\")\n",
    "df = df[[\"Date\",\"Team\",\"Player\",\"In\",\"Out\",\"Notes\"]]\n",
    "df = df[df[\"Player\"].str.istitle()].reset_index(drop=True)\n",
    "df[\"playerID\"] = df[\"Player\"].map(pID_dict)\n",
    "df1 = df.copy()\n",
    "df1[\"playerID\"][df[\"playerID\"].isna()] = df[\"Player\"][df[\"playerID\"].isna()].apply(lambda x: get_missing_pId(x,player_dict))\n",
    "df1[\"playerID\"] = df1[\"playerID\"].astype(int)\n",
    "df1[\"Date\"] = pd.to_datetime(df1[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df1.insert(2,\"playerID\",df1.pop(\"playerID\"))\n",
    "df2 = pd.concat([df0,df1]).reset_index(drop=True)\n",
    "df3 =df2[~df2.duplicated(keep='last')].reset_index(drop=True)\n",
    "df3 = df3[~df3[\"Notes\"].str.contains(\"fine\",case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('../fdata/NBA_prosptran_injuries_2023.csv', index=False)\n",
    "df3.to_parquet('../fdata/NBA_prosptran_injuries_2023.parquet')\n",
    "df3.to_csv('../../repos/csv/NBA_prosptran_injuries_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df3.query(\"Player == 'Tyler Herro'\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did Herro Miss the game on 1st November?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_date = pd.to_datetime(dt.date(2023,11,1))\n",
    "dfp[\"Comp\"] = dfp[\"Date\"] <= game_date\n",
    "idxi = dfp[dfp[\"Comp\"]].index\n",
    "if len(idxi) > 0:\n",
    "    idx = idxi[-1]\n",
    "    missed_game = dfp[\"Out\"].loc[idx]\n",
    "else:\n",
    "    missed_game = False\n",
    "missed_game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older Seasons Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_pId(player,player_dict):\n",
    "    pId = process.extract(player,player_dict,limit=1)[0][2]\n",
    "    return pId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @retry(stop=stop_after_attempt(5), wait=wait_fixed(0.6))\n",
    "def update_injury_data(year):\n",
    "\n",
    "    header = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "    start_date = f\"{year}-07-01\"\n",
    "    end_date = f\"{year+1}-06-30\"\n",
    "    try:\n",
    "        df0 = pd.read_parquet(data_DIR + f'NBA_prosptran_injuries_{year}.parquet')\n",
    "        start_date = (df0[\"Date\"].iloc[-1] + dt.timedelta(days=-1)).strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        df0 = pd.DataFrame()\n",
    "        \n",
    "    print(start_date)\n",
    "    url = f\"https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate={start_date}&EndDate={end_date}&ILChkBx=yes&InjuriesChkBx=yes&PersonalChkBx=yes&Submit=Search\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    print(response) # Response [200] means it went through\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    df_first_page = pd.read_html(url,storage_options=header)\n",
    "    df_first_page = df_first_page[0]\n",
    "    df_first_page.drop([0], inplace = True)\n",
    "    df_first_page[2]=df_first_page[2].str[2:] # \"Acquired\" column\n",
    "    df_first_page[3]=df_first_page[3].str[2:] # \"Relinquished\" column\n",
    "    df_first_page.columns = ['Date','Team','Acquired','Relinquished','Notes']\n",
    "    dfa = []\n",
    "    dfa.append(df_first_page)\n",
    "    for i in tqdm(range(4,len(soup.findAll('a'))-4)): #'a' tags are for links\n",
    "        for kk in Retrying(wait=wait_fixed(20)):\n",
    "            try: \n",
    "                tic = time.perf_counter()\n",
    "                one_a_tag = soup.findAll('a')[i]\n",
    "                link = one_a_tag['href']\n",
    "                download_url = 'https://www.prosportstransactions.com/basketball/Search/'+ link\n",
    "                # print(download_url)\n",
    "                dfs = pd.read_html(download_url, storage_options=header)\n",
    "                df = dfs[0]\n",
    "                df.drop([0], inplace = True)\n",
    "                df[2]=df[2].str[2:] # \"Acquired\" column\n",
    "                df[3]=df[3].str[2:] # \"Relinquished\" column\n",
    "                df.columns = ['Date','Team','Acquired','Relinquished','Notes']\n",
    "                toc = time.perf_counter()\n",
    "                if (toc - tic) >10:\n",
    "                    raise Exception(\"Website Timeout\")\n",
    "                time.sleep(0.2)\n",
    "                dfa.append(df)\n",
    "                break\n",
    "            except Exception as error:\n",
    "                 print(download_url)\n",
    "                 print(error)\n",
    "                 continue\n",
    "\n",
    "    df1 = pd.concat(dfa)\n",
    "    df = df1.copy()\n",
    "    acq = df['Acquired']\n",
    "    rel = df['Relinquished']\n",
    "    df['Acquired'] = np.where(\n",
    "        acq.str.contains('/'), acq.str.split('/ ').str[1], acq)\n",
    "    df['Relinquished'] = np.where(\n",
    "        rel.str.contains('/'), rel.str.split('/ ').str[1], rel)\n",
    "\n",
    "    # Remove instances where value is like \"(some text)\"\n",
    "    df['Acquired'] = df.Acquired.str.replace(\n",
    "        r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "    df['Relinquished'] = df.Relinquished.str.replace(\n",
    "        r\"[\\(\\[].*?[\\)\\]]\", \"\")\n",
    "    df[\"In\"] = ~df[\"Acquired\"].isna()\n",
    "    df[\"Out\"] = ~df[\"Relinquished\"].isna()\n",
    "    df[\"Player\"] =  (df[\"Acquired\"]*~df[\"Acquired\"].isna()).fillna(\"\") +\\\n",
    "                    (df[\"Relinquished\"]*~df[\"Relinquished\"].isna()).fillna(\"\")\n",
    "    df = df[[\"Date\",\"Team\",\"Player\",\"In\",\"Out\",\"Notes\"]]\n",
    "    df = df[df[\"Player\"].str.istitle()].reset_index(drop=True)\n",
    "    df[\"playerID\"] = df[\"Player\"].map(pID_dict)\n",
    "    df1 = df.copy()\n",
    "    df1[\"playerID\"][df[\"playerID\"].isna()] = df[\"Player\"][df[\"playerID\"].isna()].apply(lambda x: get_missing_pId(x,player_dict))\n",
    "    df1[\"playerID\"] = df1[\"playerID\"].astype(int)\n",
    "    df1[\"Date\"] = pd.to_datetime(df1[\"Date\"], format=\"%Y-%m-%d\")\n",
    "    df1.insert(2,\"playerID\",df1.pop(\"playerID\"))\n",
    "    df2 = pd.concat([df0,df1]).reset_index(drop=True)\n",
    "    df3 =df2[~df2.duplicated(keep='last')].reset_index(drop=True)\n",
    "    df3.to_csv(data_DIR + f'NBA_prosptran_injuries_{year}.csv', index=False)\n",
    "    df3.to_parquet(data_DIR + f'NBA_prosptran_injuries_{year}.parquet')\n",
    "\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2022,2024):\n",
    "    dfy = update_injury_data(year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba-ub9Z_EQq-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
