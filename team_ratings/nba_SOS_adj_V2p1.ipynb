{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Adjust Offensive, Defensive and Net Ratings for Strength of Schedule\n",
    "## Trying more stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "from nbafuns import *\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# model = make_pipeline(StandardScaler(with_mean=False), _RidgeGCV())\n",
    "\n",
    "teams_dict, teams_list = get_teams(league=\"NBA\")\n",
    "box_DIR = \"../data/box/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_decay(X):\n",
    "    X = X.to_numpy()\n",
    "    y = np.zeros(np.size(X))\n",
    "    for i,x in enumerate(X):\n",
    "        yj=0\n",
    "        for j in range(i):\n",
    "            xj = x-X[j]\n",
    "            yj += np.exp(-(x-X[j]))\n",
    "        y[i] = yj\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratings(season=2024,rest = 1):\n",
    "    cols = [\n",
    "        \"gameId\",\n",
    "        \"teamName\",\n",
    "        \"teamId\",\n",
    "        \"offensiveRating\",\n",
    "        \"defensiveRating\",\n",
    "        \"netRating\",\n",
    "        \"possessions\",\n",
    "    ]\n",
    "    df1 = pd.read_parquet(box_DIR + f\"NBA_Box_T_Adv_{season}.parquet\", columns=cols)\n",
    "    cols = [\"gameId\", \"team\", \"tId\", \"ORtg\", \"DRtg\", \"NRtg\", \"poss\"]\n",
    "    df1.columns = cols\n",
    "    df2 = pd.read_parquet(box_DIR + f\"NBA_Box_T_Base_{season}.parquet\")\n",
    "    df2[\"home\"] = ~df2[\"MATCHUP\"].str.contains(\"@\")\n",
    "    df2[\"tId\"] = df2[\"TEAM_ID\"]\n",
    "    df2[\"gameId\"] = df2[\"GAME_ID\"]\n",
    "    df2[\"date\"] = df2[\"GAME_DATE\"]\n",
    "    df2 = (\n",
    "            df2[[\"date\",\"gameId\", \"tId\", \"home\"]]\n",
    "            .sort_values(by=[\"date\",\"gameId\", \"tId\", \"home\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    df2[\"gameId\"] = df2[\"gameId\"].astype(int)\n",
    "    df3 = pd.merge(df1,df2)\n",
    "    df4 = df3.sort_values(by=[\"date\"])\n",
    "    df4[\"date_d\"] = (df4[\"date\"] - df4[\"date\"].iloc[0]).dt.days\n",
    "    df4g = df4.groupby(\"tId\")\n",
    "    keys = list(df4g.groups)\n",
    "    dfa = []\n",
    "    for key in keys:\n",
    "        df5 = df4g.get_group(key)\n",
    "        df5[\"rest\"] = time_decay(df5['date_d'])\n",
    "        df5[\"rest\"] = df5[\"rest\"].round(5)\n",
    "        dfa.append(df5)\n",
    "    df6 = pd.concat(dfa)\n",
    "    df7 = df6.groupby(\"gameId\")\n",
    "    df7_1 = df7.nth(0)\n",
    "    df7_2 = df7.nth(1)\n",
    "    df7_1.columns = [\"gameId\"] + [s + \"1\" for s in df7_1.columns if s != \"gameId\"]\n",
    "    df7_2.columns = [\"gameId\"] + [s + \"2\" for s in df7_2.columns if s != \"gameId\"]\n",
    "    df7_3 = pd.merge(df7_1, df7_2, on=\"gameId\")\n",
    "    df7_4 = df7.nth(1)\n",
    "    df7_5 = df7.nth(0)\n",
    "    df7_4.columns = [\"gameId\"] + [s + \"1\" for s in df7_4.columns if s != \"gameId\"]\n",
    "    df7_5.columns = [\"gameId\"] + [s + \"2\" for s in df7_5.columns if s != \"gameId\"]\n",
    "    df7_6 = pd.merge(df7_4, df7_5, on=\"gameId\")\n",
    "    df8 = pd.concat([df7_3, df7_6]).sort_values(by=\"date1\").reset_index(drop=True)\n",
    "    df9 = df8.copy()\n",
    "    df9[\"pts1\"] = df9[\"ORtg1\"] * df9[\"poss1\"] \n",
    "    df9[\"pts2\"] = df9[\"DRtg1\"] * df9[\"poss1\"]\n",
    "    off_all = (df9[\"pts1\"].sum() / df9[\"poss1\"].sum()).round(3)\n",
    "    def_all = (df9[\"pts2\"].sum() / df9[\"poss1\"].sum()).round(3)\n",
    "    df9 = df8.query(\"home1\").reset_index(drop=True)\n",
    "    df9[\"pts1\"] = df9[\"ORtg1\"] * df9[\"poss1\"] \n",
    "    df9[\"pts2\"] = df9[\"DRtg1\"] * df9[\"poss1\"]\n",
    "    off_home = (df9[\"pts1\"].sum() / df9[\"poss1\"].sum()).round(3)\n",
    "    def_home = (df9[\"pts2\"].sum() / df9[\"poss1\"].sum()).round(3)\n",
    "    off_adv = off_home - off_all\n",
    "    def_adv = def_home - def_all\n",
    "    df8[\"Prior\"] = (\n",
    "        - rest* df8[\"rest1\"]  \n",
    "        + rest* df8[\"rest2\"] \n",
    "        + np.where(df8[\"home1\"],1,-1)*off_adv\n",
    "        + np.where(df8[\"home2\"],1,-1)*def_adv\n",
    "    ).round(3)\n",
    "\n",
    "    return df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(data, results_adj):\n",
    "    data[\"pts1\"] = data[\"ORtg1\"] * data[\"poss1\"]\n",
    "    data[\"pts2\"] = data[\"DRtg1\"] * data[\"poss1\"]\n",
    "    off_prior = data.groupby([\"tId1\"])[[\"poss1\", \"pts1\"]].agg(\"sum\").reset_index()\n",
    "    def_prior = data.groupby([\"tId1\"])[[\"poss1\", \"pts2\"]].agg(\"sum\").reset_index()\n",
    "    off_prior[\"OFF\"] = off_prior[\"pts1\"] / off_prior[\"poss1\"]\n",
    "    off_prior = off_prior[[\"tId1\", \"OFF\"]]\n",
    "    def_prior[\"DEF\"] = def_prior[\"pts2\"] / def_prior[\"poss1\"]\n",
    "    def_prior = def_prior[[\"tId1\", \"DEF\"]]\n",
    "    results_net = pd.merge(off_prior, def_prior, on=[\"tId1\"])\n",
    "    results_net[\"NET\"] = results_net[\"OFF\"] - results_net[\"DEF\"]\n",
    "    results_net.rename(columns={\"tId1\": \"tId\"}, inplace=True)\n",
    "    results_net = results_net.astype(float).round(2)\n",
    "    results_net[\"tId\"] = results_net[\"tId\"].astype(int)\n",
    "    ortg_mean = data[\"pts1\"].sum() / data[\"poss1\"].sum()\n",
    "    drtg_mean = data[\"pts2\"].sum() / data[\"poss1\"].sum()\n",
    "    results_adj[\"tId\"] = results_adj[\"tId\"].astype(int)\n",
    "    results_comb = pd.merge(results_net, results_adj, on=[\"tId\"])\n",
    "    # results_comb[\"aOFF\"] = results_comb[\"aOFF\"] #+ results_comb[\"OFF\"]\n",
    "    # results_comb[\"aDEF\"] = results_comb[\"aDEF\"] #+ results_comb[\"DEF\"]\n",
    "    # results_comb[\"aNET\"] = results_comb[\"aNET\"] #+ results_comb[\"NET\"]\n",
    "    results_comb[\"oSOS\"] = results_comb[\"aOFF\"] - results_comb[\"OFF\"]\n",
    "    results_comb[\"dSOS\"] = results_comb[\"DEF\"] - results_comb[\"aDEF\"]\n",
    "    results_comb[\"SOS\"] = results_comb[\"oSOS\"] + results_comb[\"dSOS\"]\n",
    "    results_comb.iloc[:, 1:] = results_comb.iloc[:, 1:].round(1)\n",
    "    results = results_comb[\n",
    "        [\"Team\", \"OFF\", \"oSOS\", \"aOFF\", \"DEF\", \"dSOS\", \"aDEF\", \"NET\", \"SOS\", \"aNET\"]\n",
    "    ]\n",
    "    # results = results_comb[[\"Team\",\"OFF\",\"DEF\",\"NET\",\"aOFF\",\"aDEF\",\"aNET\"]]\n",
    "    results = results.sort_values(by=\"aNET\", ascending=0).reset_index(drop=True)\n",
    "    return results, ortg_mean, drtg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_teams(row_in, teams, scale):\n",
    "    t1 = row_in[0]\n",
    "    t2 = row_in[1]\n",
    "\n",
    "    rowOut = np.zeros([len(teams) * 2])\n",
    "    rowOut[teams.index(t1)] = scale\n",
    "    rowOut[teams.index(t2) + len(teams)] = scale\n",
    "\n",
    "    return rowOut\n",
    "\n",
    "\n",
    "def convert_to_matricies(possessions, name, teams, prior, scale=1):\n",
    "    # extract only the columns we need\n",
    "    # Convert the columns of player ids into a numpy matrix\n",
    "    stints_x_base = possessions[[\"tId1\", \"tId2\"]].to_numpy()\n",
    "    # Apply our mapping function to the numpy matrix\n",
    "    stint_X_rows = np.apply_along_axis(map_teams, 1, stints_x_base, teams, scale=scale)\n",
    "    # Convert the column of target values into a numpy matrix\n",
    "    # Convert the column of target values into a numpy matrix\n",
    "    stint_Y_rows_before = possessions[name].to_numpy()\n",
    "    stint_Y_rows = stint_Y_rows_before - prior\n",
    "\n",
    "    # return matricies and possessions series\n",
    "    return stint_X_rows, stint_Y_rows\n",
    "\n",
    "\n",
    "# Convert lambda value to alpha needed for ridge CV\n",
    "\n",
    "\n",
    "def lambda_to_alpha(lambda_value, samples):\n",
    "    return (lambda_value * samples) / 2.0\n",
    "\n",
    "\n",
    "# Convert RidgeCV alpha back into a lambda value\n",
    "\n",
    "\n",
    "def alpha_to_lambda(alpha_value, samples):\n",
    "    return (alpha_value * 2.0) / samples\n",
    "\n",
    "\n",
    "def calculate_netrtg(train_x, train_y, lambdas, teams_list, prior):\n",
    "    alphas = [lambda_to_alpha(l, train_x.shape[0]) for l in lambdas]\n",
    "    # create a 5 fold CV ridgeCV model. Our target data is not centered at 0, so we want to fit to an intercept.\n",
    "    clf = RidgeCV(alphas=alphas, cv=5, fit_intercept=True)\n",
    "\n",
    "    # fit our training data\n",
    "    model = clf.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "    )\n",
    "\n",
    "    # convert our list of players into a mx1 matrix\n",
    "    team_arr = np.transpose(np.array(teams_list).reshape(1, len(teams_list)))\n",
    "\n",
    "    # extract our coefficients into the offensive and defensive parts\n",
    "    coef_ = model.coef_ \n",
    "    coef_offensive_array = coef_[0 : len(teams_list)][np.newaxis].T\n",
    "    coef_defensive_array = coef_[len(teams_list) : 2 * len(teams_list)][\n",
    "        np.newaxis\n",
    "    ].T\n",
    "    # concatenate the offensive and defensive values with the playey ids into a mx3 matrix\n",
    "    team_id_with_coef = np.concatenate(\n",
    "        [team_arr, coef_offensive_array, coef_defensive_array], axis=1\n",
    "    )\n",
    "    # build a dataframe from our matrix\n",
    "    teams_coef = pd.DataFrame(team_id_with_coef)\n",
    "    intercept = model.intercept_\n",
    "    teams_coef.columns = [\"tId\", \"aOFF\", \"aDEF\"]\n",
    "    teams_coef[\"aNET\"] = teams_coef[\"aOFF\"] - teams_coef[\"aDEF\"]\n",
    "    teams_coef[\"aOFF\"] = teams_coef[\"aOFF\"] + intercept\n",
    "    teams_coef[\"aDEF\"] = teams_coef[\"aDEF\"] + intercept\n",
    "    teams_coef[\"Team\"] = teams_coef[\"tId\"].map(teams_dict)\n",
    "    results = teams_coef[[\"tId\", \"Team\", \"aOFF\", \"aDEF\", \"aNET\"]]\n",
    "    results = results.sort_values(by=[\"aNET\"], ascending=False).reset_index(drop=True)\n",
    "    return results, model, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasons = np.arange(2010,2024,1).astype(str)\n",
    "# dfa = []\n",
    "# for season in seasons:\n",
    "#     df = get_ratings(season)\n",
    "#     dfa.append(df)\n",
    "# data = pd.concat(dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_ratings(2023, rest=10)\n",
    "prior = data[\"Prior\"].to_numpy()\n",
    "train_x, train_y = convert_to_matricies(data, \"ORtg1\", teams_list, prior, scale = 1/2)\n",
    "n = 1.5\n",
    "lambdas_net = [0.001 * n, 0.005 * n, 0.01 * n]\n",
    "results_adj, model, intercept = calculate_netrtg(\n",
    "    train_x, train_y, lambdas_net, teams_list, prior\n",
    ")\n",
    "results, ortg_mean, drtg_mean = process_results(data, results_adj)\n",
    "print(intercept)\n",
    "results[[\"Team\",\"NET\",\"aNET\",\"SOS\"]].sort_values(\"SOS\",ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcvxcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = \"NET\"\n",
    "# var1 = \"OFF\"\n",
    "# var1 = \"DEF\"\n",
    "var2 = \"a\" + var1\n",
    "slope, intercept, r, p, sterr = scipy.stats.linregress(x=results[var1], y=results[var2])\n",
    "r2 = r**2\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig = sns.regplot(\n",
    "    x=var1,\n",
    "    y=var2,\n",
    "    data=results,\n",
    "    color=\"black\",\n",
    "    scatter_kws={\"color\": \"tab:blue\"},\n",
    "    ax=ax,\n",
    ")\n",
    "ax.text(0.05, 0.9, r\"$r^2=$\" + f\"{round(r2,4)}\", transform=ax.transAxes)\n",
    "ax.set_title(\"Adjusted Net Ratings vs Unadjusted\")\n",
    "plt.savefig(\"../figs/team_leaders/aNET_R2_1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"OFF_R\"] = results[\"OFF\"].rank(ascending=False).astype(int)\n",
    "results[\"DEF_R\"] = results[\"DEF\"].rank(ascending=True).astype(int)\n",
    "results[\"NET_R\"] = results[\"NET\"].rank(ascending=False).astype(int)\n",
    "results[\"aOFF_R\"] = results[\"aOFF\"].rank(ascending=False).astype(int)\n",
    "results[\"aDEF_R\"] = results[\"aDEF\"].rank(ascending=True).astype(int)\n",
    "results[\"aNET_R\"] = results[\"aNET\"].rank(ascending=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams = pd.read_csv(\"../data/NBA_teams_colors_logos.csv\")\n",
    "df_teams = df_teams.rename(columns={\"nameTeam\": \"Team\"})\n",
    "results_plot = pd.merge(results, df_teams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dav-ub9Z_EQq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
